{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['/home/bam/miniconda3/envs/bam/lib/python3.12/site-packages/pinecone_plugins'])\n",
      "Discovering subpackages in _NamespacePath(['/home/bam/miniconda3/envs/bam/lib/python3.12/site-packages/pinecone_plugins'])\n",
      "Discovering subpackages in _NamespacePath(['/home/bam/miniconda3/envs/bam/lib/python3.12/site-packages/pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "Looking for plugins in pinecone_plugins.inference\n",
      "Looking for plugins in pinecone_plugins.inference\n",
      "INFO:pinecone_plugin_interface.logging:Installing plugin inference into Pinecone\n",
      "Installing plugin inference into Pinecone\n",
      "Installing plugin inference into Pinecone\n"
     ]
    }
   ],
   "source": [
    "_=load_dotenv(find_dotenv())\n",
    "client = OpenAI()\n",
    "pc = Pinecone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.create_index(\n",
    "    name=\"hybrid\",\n",
    "    dimension=1536,\n",
    "    metric=\"dotproduct\",\n",
    "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_index = pc.Index(\"hybrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"./data1\", recursive=True).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, model=\"text-embedding-ada-002\"):\n",
    "    response = client.embeddings.create(input=texts, model=model)\n",
    "    return [data['embedding'] for data in response['data']]\n",
    "embed_model = get_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upserted vectors: 100%|██████████| 157/157 [00:09<00:00, 16.78it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import StorageContext\n",
    "\n",
    "\n",
    "vector_store = PineconeVectorStore(\n",
    "    pinecone_index=hybrid_index,\n",
    "    add_sparse_vector=True,\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, transformations=[SentenceSplitter(chunk_size=1024, chunk_overlap=20)], storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = index.as_query_engine(similarity_top_k=5, model=\"gpt-4o-mini\")\n",
    "response = query_engine.query(\"What did Pres. Nelson say about the Cardston Temple?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Pres. Nelson mentioned that when he was born, there were six functioning temples in the Church, including one in Cardston, Alberta, Canada."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"{response}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
