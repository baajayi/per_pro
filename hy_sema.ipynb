{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bam/miniconda3/envs/bam/lib/python3.12/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['/home/bam/miniconda3/envs/bam/lib/python3.12/site-packages/pinecone_plugins'])\n",
      "Discovering subpackages in _NamespacePath(['/home/bam/miniconda3/envs/bam/lib/python3.12/site-packages/pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "Looking for plugins in pinecone_plugins.inference\n",
      "INFO:pinecone_plugin_interface.logging:Installing plugin inference into Pinecone\n",
      "Installing plugin inference into Pinecone\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import yaml\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, PromptTemplate\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from IPython.display import Markdown, display\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "_=load_dotenv(find_dotenv())\n",
    "client = OpenAI()\n",
    "pc = Pinecone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_metadata_from_markdown(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "        if lines and lines[0].strip() == '---':\n",
    "            end_index = None\n",
    "            for i in range(1, len(lines)):\n",
    "                if lines[i].strip() == '---':\n",
    "                    end_index = i\n",
    "                    break\n",
    "            \n",
    "            if end_index:\n",
    "                yaml_content = ''.join(lines[1:end_index])\n",
    "                metadata = yaml.safe_load(yaml_content)\n",
    "                \n",
    "                url = metadata.get('url', \"https://www.churchofjesuschrist.org/study/general-conference/2024/04?lang=eng\")\n",
    "                author = metadata.get('author', None)\n",
    "                if author:\n",
    "                    author = author.replace('\\xa0', ' ')\n",
    "                \n",
    "                return url, author\n",
    "    \n",
    "    return \"https://www.churchofjesuschrist.org/study/general-conference/2024/04?lang=eng\", None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_metadata_extractor(file_path):\n",
    "    if file_path.endswith('.md'):\n",
    "        url, author = extract_metadata_from_markdown(file_path)\n",
    "        if url:\n",
    "            return {'url': url, 'Author': author}\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = SimpleDirectoryReader(\"data1\", file_metadata=file_metadata_extractor, recursive=True).load_data()\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1,\n",
    "    breakpoint_percentile_threshold=95,\n",
    "    embed_model=embed_model \n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_metadata_to_nodes(nodes, docs):\n",
    "    for node, doc in zip(nodes, docs):\n",
    "        node.metadata = doc.metadata\n",
    "    return nodes\n",
    "nodes_with_metadata = add_metadata_to_nodes(nodes, docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'hy-sema' already exists. Connecting to the existing index.\n"
     ]
    }
   ],
   "source": [
    "index_name = \"hy-sema\"\n",
    "\n",
    "existing_indexes = [index['name'].strip().lower() for index in pc.list_indexes()]\n",
    "index_name_normalized = index_name.strip().lower()\n",
    "\n",
    "if index_name_normalized not in existing_indexes:\n",
    "    print(f\"Index '{index_name}' does not exist. Creating a new index.\")\n",
    "    pc.create_index(index_name, dimension=1536, metric=\"dotproduct\", spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"))\n",
    "else:\n",
    "    print(f\"Index '{index_name}' already exists. Connecting to the existing index.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hy_sema_index = pc.Index(\"hy-sema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_vector_exists(pinecone_index, doc_id):\n",
    "    existing_vector = pinecone_index.fetch(ids=[doc_id])\n",
    "    return existing_vector is not None and len(existing_vector['vectors']) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bam/miniconda3/envs/bam/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import StorageContext\n",
    "\n",
    "\n",
    "vector_store = PineconeVectorStore(\n",
    "    pinecone_index=hy_sema_index,\n",
    "    add_sparse_vector=True,\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "vector_index = VectorStoreIndex(nodes=[], embed_model=embed_model, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "President Russell M. Nelson mentioned that there was a functioning temple in Cardston, Alberta, Canada, among the six temples that were operational when he was born."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# query_engine = vector_index.as_query_engine(similarity_top_k=5, model=\"gpt-4o-mini\")\n",
    "# response = query_engine.query(\"What did Pres. Nelson say about the Cardston Temple?\")\n",
    "# display(Markdown(f\"{response}\"))\n",
    "# # display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_tmpl_str =\"\"\"\n",
    "You are an AI assistant tasked with answering questions about the April 2024 General Conference of the Church of Jesus Christ of Latter-Day Saints. You will be provided with the content of the conference talks and a question to answer. Your goal is to provide accurate and relevant information based solely on the content of these talks.\n",
    "\n",
    "Here is the content of the conference talks:\n",
    "\n",
    "<conference_talks>\n",
    "{{CONFERENCE_TALKS}}\n",
    "</conference_talks>\n",
    "\n",
    "When answering questions, follow these guidelines:\n",
    "\n",
    "1. Carefully read and analyze the conference talks to find relevant information and think deeply how the relevant information can be used to answer the question.\n",
    "2. Only use information explicitly stated in the provided talks. Do not include external knowledge or personal interpretations.\n",
    "3. If the question cannot be answered using the information in the talks, state that the answer is not found in the provided content.\n",
    "4. Provide direct quotes from the talks when possible to support your answer. Use quotation marks and indicate the speaker's name for each quote.\n",
    "5. If multiple talks address the question, synthesize the information from all relevant sources.\n",
    "6. Maintain a respectful and reverent tone when discussing religious topics.\n",
    "\n",
    "Be as concise and complete as possible\n",
    "\n",
    "Use the following XML tags to structure your response:\n",
    "<answer><main_content></main_content><limitations></limitations>\n",
    "</answer>\n",
    "\n",
    "Now, please answer the following question based on the conference talks provided:\n",
    "\n",
    "<question>\n",
    "{{QUESTION}}\n",
    "</question>\"\"\"\n",
    "\n",
    "meta_tmpl = PromptTemplate(meta_tmpl_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:summary_template\": meta_tmpl}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_dict = query_engine.get_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = vector_index.as_query_engine(response_mode=\"tree_summarize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_prompt_dict(prompts_dict):\n",
    "    for k, p in prompts_dict.items():\n",
    "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
    "        display(Markdown(text_md))\n",
    "        print(p.get_template())\n",
    "        display(Markdown(\"<br><br>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:summary_template<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an AI assistant tasked with answering questions about the April 2024 General Conference of the Church of Jesus Christ of Latter-Day Saints. You will be provided with the content of the conference talks and a question to answer. Your goal is to provide accurate and relevant information based solely on the content of these talks.\n",
      "\n",
      "Here is the content of the conference talks:\n",
      "\n",
      "<conference_talks>\n",
      "{{CONFERENCE_TALKS}}\n",
      "</conference_talks>\n",
      "\n",
      "When answering questions, follow these guidelines:\n",
      "\n",
      "1. Carefully read and analyze the conference talks to find relevant information and think deeply how the relevant information can be used to answer the question.\n",
      "2. Only use information explicitly stated in the provided talks. Do not include external knowledge or personal interpretations.\n",
      "3. If the question cannot be answered using the information in the talks, state that the answer is not found in the provided content.\n",
      "4. Provide direct quotes from the talks when possible to support your answer. Use quotation marks and indicate the speaker's name for each quote.\n",
      "5. If multiple talks address the question, synthesize the information from all relevant sources.\n",
      "6. Maintain a respectful and reverent tone when discussing religious topics.\n",
      "\n",
      "Be as concise and complete as possible\n",
      "\n",
      "Use the following XML tags to structure your response:\n",
      "<answer><main_content></main_content><limitations></limitations>\n",
      "</answer>\n",
      "\n",
      "Now, please answer the following question based on the conference talks provided:\n",
      "\n",
      "<question>\n",
      "{{QUESTION}}\n",
      "</question>\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = vector_index.as_query_engine(similarity_top_k=5, model=\"gpt-4o-mini\")\n",
    "response = query_engine.query(\"What did Pres. Nelson say about the Cardston Temple?\")\n",
    "display(Markdown(f\"{response}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
